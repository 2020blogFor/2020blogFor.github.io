---
title: 'Neural Operator, Graph Kernel Network for PDEs'
date: 2020-04-25
permalink: /posts/2020/04/BO/
tags:
  - BO
---
This is a blog post credit to Yujia Huang, Zongyi Li, and Jiawei Zhao



# Introduction
It introduces our recent work that uses graph neural networks to learn 
**mappings between function spaces** and solve partial differential equations. 
You can also check out the [paper](https://arxiv.org/abs/2003.03485) for more formal derivations.

Scientific computations are expensive. 
It could take days and months for numerical solvers to simulate fluid dynamics and many-body motions. 
Because to achieve good accuracy, 
the numerical solvers need to discretize the space and time into very fine grids 
and solve a great number of equations on the grids.
Recently, people are developing data-driven methods based on machine learning techniques such as deep learning.
Instead of directly solving the problems, data-driven solvers learn from the data of the problems and solutions.
When querying new problems, data-driven solvers can directly give predictions based on the data.
Since they don't need to discretize the space into very small pieces and solve all these equations,
these data-driven solvers are usually much faster compared to traditional numerical solvers,

However, data-driven solvers are subject to the quality of the data given.
If the training data is not good enough, they can't make good predictions.
In scientific computing, usually, the training data are generated by the traditional numerical solvers.
And to generate good enough data, it still takes days and months for these numerical solvers.
Sometime, data are observed from experiments and there are just no good training data.
Especially, people consider neural networks as interpolators which may not be able to extrapolate.
It is unclear whether neural networks can generalize to unseen data. 
So if the training data are of one resolution, 
the learned solvers can only solve the problem in this specific resolution. 
In general, generalization is a crucial problem in machine learning.
It becomes a trade-off: these machine learning based methods make evaluation easier, 
but the training process could be even more painful.

To dealing with this problem, we purpose operator learning. By encoding certain structures,
we let the neural network learn the mapping of functions and generalize among different resolutions.
As a result, we can first use a numerical method generated some less-accurate, low-resolution data, 
but the learned solver is still able to give reasonable, high-resolution predictions.
In some sense, both training and evaluation can be pain-free.


